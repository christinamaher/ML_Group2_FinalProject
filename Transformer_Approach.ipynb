{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ICUTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, static_dim, d_model=64, nhead=4, num_layers=2, ff_dim=128, dropout=0.1):\n",
    "        super(ICUTransformer, self).__init__()\n",
    "\n",
    "        # Project input features to d_model dimensions\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "\n",
    "        # Positional encoding (learned, not fixed sinusoidal)\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, 48, d_model))  # assuming 48 time steps\n",
    "\n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Combine Transformer output with static features\n",
    "        self.combined_proj = nn.Linear(d_model + static_dim, 64)\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x_seq, x_static):\n",
    "        \"\"\"\n",
    "        x_seq: Tensor of shape (batch_size, time_steps=48, input_dim)\n",
    "        x_static: Tensor of shape (batch_size, static_dim)\n",
    "        \"\"\"\n",
    "        x = self.input_proj(x_seq)             # → (batch, time, d_model)\n",
    "        x = x + self.pos_encoder               # add positional encoding\n",
    "        x = self.transformer_encoder(x)        # → (batch, time, d_model)\n",
    "        x = x.mean(dim=1)                      # global average pooling over time\n",
    "\n",
    "        x_combined = torch.cat([x, x_static], dim=1)  # concatenate with static features\n",
    "        x_out = self.combined_proj(x_combined)\n",
    "        return self.classifier(x_out)          # → output probability\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# X_time_series: [num_samples, 48, 10]\n",
    "# X_static: [num_samples, 8]\n",
    "# y: [num_samples]\n",
    "\n",
    "X_seq_tensor = torch.tensor(normalized_tensor, dtype=torch.float32)\n",
    "X_static_tensor = torch.tensor(static_encoded_df.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)  # [N, 1]\n",
    "\n",
    "dataset = TensorDataset(X_seq_tensor, X_static_tensor, y_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32)\n",
    "\n",
    "model = ICUTransformer(input_dim=10, static_dim=8)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for x_seq, x_static, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_seq, x_static)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for x_seq, x_static, y in val_loader:\n",
    "            outputs = model(x_seq, x_static)\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_pred.extend(outputs.cpu().numpy())\n",
    "\n",
    "    y_pred_label = [1 if p > 0.5 else 0 for p in y_pred]\n",
    "    val_acc = accuracy_score(y_true, y_pred_label)\n",
    "    val_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} — Loss: {avg_loss:.4f}, Val Acc: {val_acc:.4f}, ROC-AUC: {val_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
